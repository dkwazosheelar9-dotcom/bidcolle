# -*- coding: utf-8 -*-
import json
import time
import requests
import logging
import pymysql
import os

# ================= 1. ä¿®å¤ä»£ç†æŠ¥é”™ (å¿…é¡»æ”¾åœ¨æœ€å‰é¢) =================
# å¼ºåˆ¶ç§»é™¤ç³»ç»Ÿä»£ç†è®¾ç½®ï¼Œç¡®ä¿ requests ç›´è¿é˜¿é‡Œäº‘
os.environ['NO_PROXY'] = '*'
for k in ["http_proxy", "https_proxy", "HTTP_PROXY", "HTTPS_PROXY"]:
    if k in os.environ:
        os.environ.pop(k)

# å¯¼å…¥é‡‡é›†é€»è¾‘
from src import config
from src.bid_mysql import get_db_connection
from src.auto_fetch import process_task as run_auto_fetch

# ================= 2. é…ç½®åŒºåŸŸ =================
LLM_API_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
LLM_API_KEY = "sk-4b983791a8cb4693b853b389e5cceebe"
LLM_MODEL = "qwen-plus"

# æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ç™½åå•æ–¹æ³•
VALID_METHODS = ['get_html', 'post_html', 'get_json', 'post_json', 'selenium']

# 3. å®šä¹‰ JSON Schema (ä¸¥æ ¼æ¨¡å¼)
SPIDER_RULE_SCHEMA = {
    "name": "spider_rule_config",
    "strict": True,
    "schema": {
        "type": "object",
        "properties": {
            "webname": { "type": "string", "description": "æ ¼å¼ï¼šç½‘ç«™å - æ ç›®å" },
            "webadd": { "type": "string" },
            "href": { "type": "string" },
            "quchong_weburl": { "type": "string" },
            "is_active": { "type": "integer", "const": 1 },
            "post_headers": {
                "type": "object",
                "properties": {
                    "Accept": {"type": "string"},
                    "Host": {"type": "string"},
                    "Referer": {"type": "string"},
                    "User-Agent": {"type": "string"}
                },
                "required": ["Accept", "Host", "Referer", "User-Agent"],
                "additionalProperties": True
            },
            "list_method": { "type": "string", "enum": ["get_html", "post_html"], "default": "get_html" },
            "detail_method": { "type": "string", "enum": ["get_html", "post_html"], "default": "get_html" },
            "find_list": { "type": "string", "description": "è¿”å›ResultSetçš„evalä»£ç " },
            "find_href": { "type": "string", "description": "è¯¦æƒ…é“¾æ¥æå–è§„åˆ™" },
            "find_title": { "type": "string" },
            "find_pub_time": { "type": "string" },
            "find_content": { "type": "string" },
            "post_payload1": { "type": ["string", "null"] },
            "post_content_href": { "type": ["string", "null"] }
        },
        "required": [
            "webname", "webadd", "href", "quchong_weburl", "is_active",
            "post_headers", "list_method", "detail_method",
            "find_list", "find_href", "find_title", "find_pub_time", "find_content",
            "post_payload1", "post_content_href"
        ],
        "additionalProperties": False
    }
}

# 4. å‡çº§ç‰ˆ Prompt (åŒ…å«åµŒå¥—åˆ—è¡¨æ¡ˆä¾‹)
PROMPT_TEMPLATE = """
# Role
Python çˆ¬è™«æ¶æ„å¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æ HTML ç”Ÿæˆ BeautifulSoup æå–è§„åˆ™ã€‚

# âš ï¸ Core Strategy: Pattern Recognition (æ¨¡å¼è¯†åˆ«)
è¯·æ³¨æ„ç”„åˆ«ä»¥ä¸‹ä¸‰ç§å¸¸è§åˆ—è¡¨æ¨¡å¼ï¼š
1. **æ ‡å‡†åˆ—è¡¨ (UL/LI)**: `soup.find('ul', class_='news').find_all('li')`
2. **è¡¨æ ¼åˆ—è¡¨ (Table/TR)**: `soup.find('table', id='list').find_all('tr')`
3. **DIV å—çº§åˆ—è¡¨**: `soup.find('div', class_='list').find_all('div', class_='item')`

# ğŸŒ° Deep Nested Example (æ·±å±‚åµŒå¥— - é‡ç‚¹å…³æ³¨)
å½“ HTML ç»“æ„ä¸º `li > div > a` æ—¶ï¼š
```html
<ul class="list">
  <li class="item">
     <div class="title-box"> <a href="...">æ ‡é¢˜</a> </div>
     <span class="date">...</span>
  </li>
</ul>

ä½ çš„è§„åˆ™å¿…é¡»ç©¿é€ä¸­é—´å±‚ï¼š

    "find_list": "soup.find('ul', class_='list').find_all('li')"

    "find_href": "item.find('div', class_='title-box').find('a')['href']" <-- æ³¨æ„è¿™é‡Œ!

    "find_title": "item.find('div', class_='title-box').find('a').get_text(strip=True)"

Constraints (çº¦æŸ)

    find_list: å®šä½åˆ°æœ€å°å…¬å…±çˆ¶çº§å®¹å™¨ã€‚

    webname: "ç½‘ç«™å - æ ç›®å"ã€‚

    Anti-Interference: ä¸¥ç¦æŠ“å– nav, sidebar, footer, related åŒºåŸŸã€‚

Input Data

"""

def check_db_schema():
    conn = get_db_connection()
    try:
        with conn.cursor() as cursor:
            cursor.execute("DESCRIBE t_bidcollect_one_level_page")
    except Exception:
        pass
    finally:
        conn.close()


def clean_rule_data(rule_data):
    """æ•°æ®æ¸…æ´—: ç©ºå€¼è½¬None, å¼ºåˆ¶å°å†™, ç§»é™¤Methodå¼•å·"""
    nullable_fields = [
        'post_payload1', 'post_content_href',
        'find_list', 'find_href', 'find_title', 'find_pub_time', 'find_content'
    ]

    # 1. æ¸…æ´—ç©ºå­—ç¬¦ä¸²
    for key, val in rule_data.items():
        if key in nullable_fields and isinstance(val, str):
            val = val.strip()
            if not val or val.lower() in ['null', 'none']:
                rule_data[key] = None

    # 2. å¼ºåˆ¶æ¸…æ´— Method å­—æ®µ
    for m in ['list_method', 'detail_method']:
        val = str(rule_data.get(m, 'get_html')).strip().lower()
        val = val.replace("'", "").replace('"', "")
        if val not in VALID_METHODS:
            val = 'get_html'
        rule_data[m] = val

    return rule_data


def call_llm_api(input_json_str):
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {LLM_API_KEY}"
    }

    payload = {
        "model": LLM_MODEL,
        "messages": [
            {"role": "system",
             "content": "You are a crawler expert. Handle nested tags (e.g. li > div > a). Output JSON."},
            {"role": "user", "content": PROMPT_TEMPLATE + input_json_str}
        ],
        "temperature": 0.01,
        "response_format": {"type": "json_schema", "json_schema": SPIDER_RULE_SCHEMA}
    }

    try:
        response = requests.post(LLM_API_URL, headers=headers, json=payload, timeout=60)
        if response.status_code != 200:
            logger.error(f"API Error: {response.text}")
            return None

        content = response.json()['choices'][0]['message']['content']
        rule_json = json.loads(content)

        # å¼ºåˆ¶è¦†ç›– URL (é˜²æ­¢ AI å¹»è§‰)
        input_data = json.loads(input_json_str)
        if input_data.get('url'):
            for f in ['href', 'webadd', 'quchong_weburl']:
                rule_json[f] = input_data['url']

        return clean_rule_data(rule_json)
    except Exception as e:
        logger.error(f"API Exception: {e}")
        return None


def save_rule_to_db(rule_data, conn):
    """æ™ºèƒ½å…¥åº“é€»è¾‘: å°è¯•å®Œæ•´æ’å…¥ï¼Œè‹¥æŠ¥Data truncatedåˆ™é™çº§æ’å…¥"""
    post_headers_str = json.dumps(rule_data.get('post_headers', {}), ensure_ascii=False)

    # æ–¹æ¡ˆ A: å®Œæ•´ SQL
    sql_full = """
        INSERT INTO t_bidcollect_one_level_page (
            webname, webadd, href, quchong_weburl, 
            is_active, list_method, detail_method,
            find_list, find_href, find_title, find_pub_time, find_content,
            post_headers, post_payload1, post_content_href,
            create_time, update_time
        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW(), NOW())
    """
    params_full = (
        rule_data.get('webname'), rule_data.get('webadd'), rule_data.get('href'), rule_data.get('quchong_weburl'),
        1,
        rule_data.get('list_method'), rule_data.get('detail_method'),
        rule_data.get('find_list'), rule_data.get('find_href'), rule_data.get('find_title'),
        rule_data.get('find_pub_time'), rule_data.get('find_content'),
        post_headers_str, rule_data.get('post_payload1'), rule_data.get('post_content_href')
    )

    try:
        with conn.cursor() as cursor:
            cursor.execute(sql_full, params_full)
        conn.commit()
        logger.info(f"âœ… å…¥åº“æˆåŠŸ: {rule_data.get('webname')}")
        return True
    except pymysql.err.DataError as e:
        # æ•è· Error 1265: Data truncated
        if e.args[0] == 1265:
            logger.warning(f"âš ï¸ æ•è·æˆªæ–­é”™è¯¯, æ­£åœ¨é™çº§é‡è¯• (å¿½ç•¥ Method å­—æ®µ)...")
            return save_rule_fallback(rule_data, conn, post_headers_str)
        else:
            logger.error(f"âŒ æ•°æ®é”™è¯¯: {e}")
            conn.rollback()
            return False
    except pymysql.err.IntegrityError:
        logger.warning(f"âš ï¸ è·³è¿‡é‡å¤: {rule_data.get('href')}")
        return True
    except Exception as e:
        logger.error(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
        conn.rollback()
        return False


def save_rule_fallback(rule_data, conn, post_headers_str):
    # æ–¹æ¡ˆ B: é™çº§ SQL (ä¸å« list_method, detail_method)
    sql_safe = """
        INSERT INTO t_bidcollect_one_level_page (
            webname, webadd, href, quchong_weburl, 
            is_active,
            find_list, find_href, find_title, find_pub_time, find_content,
            post_headers, post_payload1, post_content_href,
            create_time, update_time
        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW(), NOW())
    """
    params_safe = (
        rule_data.get('webname'), rule_data.get('webadd'), rule_data.get('href'), rule_data.get('quchong_weburl'),
        1,
        rule_data.get('find_list'), rule_data.get('find_href'), rule_data.get('find_title'),
        rule_data.get('find_pub_time'), rule_data.get('find_content'),
        post_headers_str, rule_data.get('post_payload1'), rule_data.get('post_content_href')
    )
    try:
        with conn.cursor() as cursor:
            cursor.execute(sql_safe, params_safe)
        conn.commit()
        logger.info(f"âœ… [é™çº§æ¨¡å¼] å…¥åº“æˆåŠŸ")
        return True
    except Exception as e:
        logger.error(f"âŒ [é™çº§æ¨¡å¼] ä¾ç„¶å¤±è´¥: {e}")
        conn.rollback()
        return False


def main():
    check_db_schema()

    logger.info("Step 1: è¿è¡Œæ™ºèƒ½é‡‡é›† (Auto Fetch)...")
    run_auto_fetch()

    conn = get_db_connection()
    try:
        with conn.cursor() as cursor:
            sql = "SELECT id, url, list_html, detail_html FROM t_static_sample_task WHERE status = 1"
            cursor.execute(sql)
            tasks = cursor.fetchall()

        logger.info(f"ğŸš€ å¾…ç”Ÿæˆè§„åˆ™ä»»åŠ¡æ•°: {len(tasks)}")

        for task in tasks:
            task_id = task['id']
            url = task['url']
            logger.info(f"Processing Task {task_id}: {url}")

            # å‡†å¤‡æ•°æ® (æˆªå–å‰ 25k å­—ç¬¦)
            input_data = {
                "url": url,
                "list_html": task['list_html'][:25000],
                "detail_html": task['detail_html'][:25000] if task['detail_html'] else ""
            }

            # è°ƒç”¨ AI
            rule_json = call_llm_api(json.dumps(input_data, ensure_ascii=False))

            if rule_json:
                success = save_rule_to_db(rule_json, conn)
                # åªæœ‰å…¥åº“æˆåŠŸ(åŒ…å«è·³è¿‡é‡å¤)æ‰æ›´æ–°çŠ¶æ€
                if success:
                    with conn.cursor() as cursor:
                        cursor.execute("UPDATE t_static_sample_task SET status = 2 WHERE id = %s", (task_id,))
                    conn.commit()

            time.sleep(1)  # é¿å… API é€Ÿç‡é™åˆ¶

    finally:
        conn.close()
        logger.info("ğŸ‰ æ‰€æœ‰æµç¨‹æ‰§è¡Œå®Œæ¯•")


if __name__ == "__main__":
    main()